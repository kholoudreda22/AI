# -*- coding: utf-8 -*-
"""project after merging it.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K2beAlBqL3Q7ecOGaIgiVQCRwSZ7h3ql
"""

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d uciml/sms-spam-collection-dataset

!unzip /content/sms-spam-collection-dataset.zip

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

df=pd.read_csv("spam.csv", encoding='latin-1')
df

#checking for missing values
df.isnull().sum()

# Dropping the unnecessary columns
df_cleaned = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], errors='ignore')

# Renaming the columns for better clarity
df_cleaned.columns = ['label', 'message']

# Display the first few rows of the cleaned dataset
df_cleaned.head()

sns.heatmap(df.isnull(), cmap='viridis')
plt.show()
df
#so no missing data

# Checking for outliers
# To calculate length of message
df['message_length'] = df['message'].apply(len)
print(df['message_length'].head())
print(len(df['message_length']))

# Determine the quartiles
Q1 = df['message_length'].quantile(0.25)
Q3 = df['message_length'].quantile(0.75)
IQR = Q3 - Q1

# Identify outliers based on the IQR and a fixed length threshold
fixed_length_threshold = 200
outliers = df[(df['message_length'] < (Q1 - 1.5 * IQR)) | (df['message_length'] > (Q3 + 1.5 * IQR)) | (df['message_length'] > fixed_length_threshold)]
print(outliers)

# Remove outliers
df_cleaned = df[~((df['message_length'] < (Q1 - 1.5 * IQR)) | (df['message_length'] > (Q3 + 1.5 * IQR)) | (df['message_length'] > fixed_length_threshold))]

# Plot the box plot
sns.boxplot(x=df_cleaned['message_length'])
plt.xlabel('Length of Message')
plt.title('Box Plot of Message Length (Without Outliers)')
plt.show()

# Text preprocessing
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
# Import necessary libraries
stop_words = set(nltk.corpus.stopwords.words('english'))  # Set of English stopwords
stemmer = nltk.stem.PorterStemmer()  # Initialize PorterStemmer for stemming
lemmatizer = nltk.stem.WordNetLemmatizer()  # Initialize WordNetLemmatizer for lemmatization
def clean_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove digits from the text
    text = re.sub(r'\d+', '', text)
    # Remove punctuation from the text
    text = re.sub(r'[^\w\s]', '', text)
    # Tokenize the text into words
    tokens = nltk.word_tokenize(text)
    # Remove stopwords from the tokens
    tokens = [word for word in tokens if word not in stop_words]
    # Apply stemming to the tokens
    tokens = [stemmer.stem(word) for word in tokens]
    # Apply lemmatization to the tokens
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)  # Join the tokens back into a single string

plt.figure(figsize=(6, 4))
sns.countplot(x='label', data=df)
plt.title('Distribution of Spam and Ham Messages')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

# Encoding the labels (spam = 1, ham = 0)
from sklearn.preprocessing import LabelEncoder # import the LabelEncoder class
label_encoder = LabelEncoder()
df_cleaned['label'] = label_encoder.fit_transform(df_cleaned['label'])

# Convert the text messages into numerical format using TF-IDF
message = TfidfVectorizer(stop_words='english', max_df=0.95)
X = message.fit_transform(df_cleaned['message'])
# Target variable
# Use df_cleaned instead of df to ensure consistent number of samples
y = df_cleaned['label']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

model = SVC()
model.fit(X_train, y_train)

print('accuracy = ' + str(model.score(X_train, y_train)))

y_pred = model.predict(X_test)

# Calculate evaluation metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Precision : ", precision)
print("Recall : ", recall)
print("F1-score : ", f1)
print("Confusion Matrix:\n", conf_matrix)